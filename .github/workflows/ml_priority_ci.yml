name: Train, validate & bias-check priority model

on:
  push:
    branches: [ main ]
    paths:
      - "ml_prioritization_dashboard/**"
      - ".github/workflows/ml_priority_ci.yml"
  workflow_dispatch:

jobs:
  train-validate-bias-check:
    runs-on: ubuntu-latest

    env:
      GCP_PROJECT: boston311-mlops
      BQ_LOCATION: US
      MODEL_REGISTRY_BUCKET: boston311-ml-model-registry
      TRAIN_FEATURE_TABLE: tbl_train_features

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Write GCP service account key
        env:
          GCP_SA_KEY: ${{ secrets.GCP_SA_KEY }}
        run: |
          mkdir -p secrets
          echo "$GCP_SA_KEY" > secrets/bq-dashboard-ro.json
          echo "Wrote service account key to secrets/bq-dashboard-ro.json"
          ls -R secrets

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r ml_prioritization_dashboard/requirements.txt

      - name: Train priority model (with MLflow + registry push)
        run: |
          python ml_prioritization_dashboard/train_priority_xgb.py

      - name: Check model quality thresholds (rollback gate)
        run: |
          python - << 'EOF'
          import json
          from pathlib import Path

          report_path = Path("ml_prioritization_dashboard/models/model_report.json")
          if not report_path.exists():
              raise SystemExit("model_report.json not found at ml_prioritization_dashboard/models/model_report.json")

          report = json.loads(report_path.read_text())

          test_pr_auc = report.get("test_pr_auc")
          test_roc_auc = report.get("test_roc_auc")
          test_f1 = report.get("test_f1")

          print(f"test_pr_auc={test_pr_auc}, test_roc_auc={test_roc_auc}, test_f1={test_f1}")

          MIN_PR_AUC = 0.90
          MIN_ROC_AUC = 0.80
          MIN_F1 = 0.90

          if test_pr_auc is None or test_roc_auc is None or test_f1 is None:
              raise SystemExit("Missing one or more test_* metrics in model_report.json")

          if not (test_pr_auc >= MIN_PR_AUC and test_roc_auc >= MIN_ROC_AUC and test_f1 >= MIN_F1):
              print("[FAIL] Model does not meet minimum quality thresholds; treating as failed run.")
              raise SystemExit(1)

          print("[OK] Model meets minimum quality thresholds; safe to keep as latest version.")
          EOF

      - name: Run bias analysis (Fairlearn slicing)
        run: |
          python ml_prioritization_dashboard/bias_check.py

      - name: Check bias gaps vs threshold (warnings only, no CI fail)
        run: |
          python - << 'EOF'
          import json
          import pathlib

          model_dir = pathlib.Path("ml_prioritization_dashboard/models")
          path = model_dir / "bias_report.json"

          if not path.exists():
              print(f"[WARN] No bias_report.json found at {path}; skipping bias gap checks.")
              raise SystemExit(0)

          data = json.loads(path.read_text())

          # All *_gap keys are top-level (e.g., neighborhood_accuracy_gap, reason_f1_gap, etc.)
          gap_keys = [k for k in data.keys() if k.endswith("_gap")]
          if not gap_keys:
              print("No *_gap keys found in bias report; please align with your schema.")
              raise SystemExit(0)

          allowed_gap = 0.20
          failed = False

          print(f"Checking bias gaps against threshold={allowed_gap:.2f}...\n")
          for k in sorted(gap_keys):
              val = data[k]
              print(f"{k}={val}")
              if val is None:
                  continue
              if val > allowed_gap:
                  print(f"WARNING: {k} exceeds allowed gap of {allowed_gap:.2f}")
                  failed = True

          if failed:
              print(
                  "\n[WARN] One or more bias gaps exceed the configured threshold. "
                  "For this project, we are treating this as a WARNING ONLY: "
                  "pipeline continues, but fairness issues must be documented in the report."
              )
          else:
              print("\n[OK] All tracked bias gaps are within the allowed threshold.")

          # IMPORTANT: do not fail CI on bias gaps; just warn.
          raise SystemExit(0)
          EOF

      - name: Send CI summary email
        if: always()
        uses: dawidd6/action-send-mail@v6
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.MAIL_USERNAME }}
          password: ${{ secrets.MAIL_PASSWORD }}
          subject: >
            [boston-311-ai-system] Priority model CI run - ${{ job.status }}
          to: aidevadarshini2002@gmail.com, harishvtcp@gmail.com
          from: ${{ secrets.MAIL_USERNAME }}
          secure: true
          body: |
            GitHub Actions run for boston-311-ai-system: ${{ github.workflow }}
            Status: ${{ job.status }}
            Run URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

